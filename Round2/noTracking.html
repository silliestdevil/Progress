<!-- MEDIAPIPE FACE LANDMARK DETECTION -->

<html>
  <head>
    <!-- updated code -->
    <script src="landmark.js" type="text/javascript"></script> 
    <script
      src="https://cdn.jsdelivr.net/npm/p5@1.8.0/lib/p5.js"
      type="text/javascript"
    ></script>
  </head>
  <!-- no scroll and black background -->
  <body style=" overflow-y: hidden;  overflow-x: hidden; background-color: rgb(0, 0, 0);">
<script>
// p5.js interface to Google MediaPipe Landmark Tracking 
// Tracks 478 points on the face, and calculates 52 face metrics.
// See https://mediapipe-studio.webapps.google.com/home
// Uses p5.js v.1.8.0 + MediaPipe v.0.10.7
// By Golan Levin, version of 10/29/2023
// Huge thanks to Orr Kislev, who made it work in p5's global mode!
// Based off of: https://editor.p5js.org/golan/sketches/0yyu6uEwM

// Don't change the names of these global variables.
let myFaceLandmarker;
let faceLandmarks;
let myCapture;
let lastVideoTime = -1;
let extra;

let noFaceDetectedStartTime = null;
const NO_FACE_DETECTED_THRESHOLD = 30000;

let eyebrowRaised = false; // Flag to track if eyebrow is raised

let tryingToNavigate = false; //for loading next html

// Works best with just one or two sets of landmarks.
const trackingConfig = {
  doAcquireFaceMetrics: true,
  cpuOrGpuString: "GPU", /* "GPU" or "CPU" */
  maxNumFaces: 1, // number of faces to track
};

//------------------------------------------
//loads the mediapipe Facelandmarker and sets up thhe fileset resolver for vision tasks
async function preload() {
  const mediapipe_module = await import('https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/vision_bundle.js');
  FaceLandmarker = mediapipe_module.FaceLandmarker;
  FilesetResolver = mediapipe_module.FilesetResolver;
  
  const vision = await FilesetResolver.forVisionTasks(
    "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.7/wasm"
  );
  
  // Face Landmark Tracking:
  // https://codepen.io/mediapipe-preview/pen/OJBVQJm
  // https://developers.google.com/mediapipe/solutions/vision/face_landmarker
	myFaceLandmarker = await FaceLandmarker.createFromOptions(vision, {
		numFaces: trackingConfig.maxNumFaces,
		runningMode: "VIDEO",
		outputFaceBlendshapes:trackingConfig.doAcquireFaceMetrics,
		baseOptions: {
			delegate: trackingConfig.cpuOrGpuString,
			modelAssetPath:
				"https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task",
		},
	});
}

//------------------------------------------
//function to continously process video frames from the webcam using 'requestAnimationFrame'
async function predictWebcam() {
  let startTimeMs = performance.now();
  if (lastVideoTime !== myCapture.elt.currentTime) {
    if (myFaceLandmarker) {
      faceLandmarks = myFaceLandmarker.detectForVideo(myCapture.elt,startTimeMs);
    }
    lastVideoTime = myCapture.elt.currentTime;
  }
  window.requestAnimationFrame(predictWebcam);
}

//------------------------------------------
///setup function: Initializes the canvas and sets up the webcam capture
function setup() {
  createCanvas(800, 600);
  myCapture = createCapture(VIDEO);
  myCapture.size(320, 240);
  myCapture.hide();
}

function draw() { 
  let faceDetected = false;
  background("black");
  //call all functions and extra canvas
  predictWebcam(); 
  
  if (faceDetected) {
    noFaceDetectedStartTime = null; // Reset timer when a face is detected
  } else {
    if (!noFaceDetectedStartTime) {
      noFaceDetectedStartTime = millis(); // Start timer when no face is detected
    } else if (millis() - noFaceDetectedStartTime > NO_FACE_DETECTED_THRESHOLD) {
      if (!tryingToNavigate) {
        console.log("No face detected for more than 30 seconds. Redirecting...");
        window.location.href = "index.html"; // Redirect to the next interface
        tryingToNavigate = true; // Mark navigation as initiated
      }
    }
  }
}







 
</script>
  </body>
</html>